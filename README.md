# Tooling to Build a Database of SMT-LIB Problems

This repository contains Python scripts that can be used to build a database
from a folder that contains a copy of the SMT-LIB benchmark library.  Such a
database can than be used to quickly select benchmarks for experiments, or to
study the benchmark library itself.

The database will also store results large-scale evaluations, such as 
SMT-COMP.  This will allow us to track benchmark difficulty over time.

## Webapp

There is a simple webapp to view benchmark data.  It can best started
locally using Docker.

To run the Docker container first execute
> docker build -t smtlib-db .
to build the Docker image.
To start the image run:
> docker run --rm -it -p 8000:5000 --name smtlib-db-container smtlib-db
Afterwards the webapp should be available at "http://localhost:8000".

## Database Scheme

The scheme is not yet fixed, and can evolve as we implement features.
The SMT-LIB folder structure follows the scheme `[LOGIC]/[DATE]-[BENCHMARKSET]/[FILENAME]`.

```sql
-- One row for each benchmark file.
CREATE TABLE Benchmarks(
        id INTEGER PRIMARY KEY,
        name TEXT NOT NULL, -- File path after the family
        family INT, -- Reference to the family of the benchmark
        logic NVARCHAR(100) NOT NULL, -- Logic string
        isIncremental BOOL, -- True if benchmark is in incremental folder
        size INT, -- Size of the benchmark file in bytes
        compressedSize INT, -- Size in bytes after compression with zstd
        license INT, -- Reference to license of the benchmark
        generatedOn DATETTIME, -- 'Generated on' field of the :source header.
        generatedBy TEXT, -- 'Generated by' field of the :source header.
        generator TEXT, -- 'Generator' field of the :source header.
        application TEXT, -- 'Application' field of the :source header.
        description TEXT, -- Text of the :source header after standard fields.
        category TEXT, -- Either 'industrial', 'crafted', or 'random'.
        passesDolmen BOOL, -- The Dolmen checker reports no error.
        passesDolmenStrict BOOL, -- Dolmen with '--strict=true' reports no error.
        queryCount INT NOT NULL, -- Number of (check-sat) calls in the benchmark.
        FOREIGN KEY(family) REFERENCES Families(id)
        FOREIGN KEY(license) REFERENCES Licenses(id)
        FOREIGN KEY(logic) REFERENCES Logics(logic)
    );
-- One row for each (check-sat) call in a benchmark.
CREATE TABLE Queries(
        id INTEGER PRIMARY KEY,
        benchmark INT, -- Reference to the benchmark this query belongs to.
        index INT, -- Index of the query in the benchmark.  Counted from 1.
        normalizedSize INT, -- Size in bytes of the query.
        compressedSize INT, -- Size in bytes of the query compressed with zstd.
        assertsCount INT, -- Number of asserts in the query.
        declareFunCount INT,
        declareConstCount INT,
        declareSortCount INT,
        defineFunCount INT,
        defineFunRecCount INT,
        constantFunCount INT,
        defineSortCount INT,
        declareDatatypeCount INT,
        maxTermDepth INT,
        status TEXT, -- Status of the query as declared in the benchmark.
        inferredStatus TEXT,  -- Status derived from evaluation results.
        FOREIGN KEY(benchmark) REFERENCES Benchmarks(id)
    );
-- Benchmark sets
CREATE TABLE Families(
        id INTEGER PRIMARY KEY,
        name NVARCHAR(100) NOT NULL,
        folderName TEXT NOT NULL,
        date DATE,
        firstOccurrence DATE,
        benchmarkCount INT NOT NULL,
        UNIQUE(folderName)
    );
CREATE TABLE TargetSolvers(
        id INTEGER PRIMARY KEY,
        benchmark INTEGER NOT NULL,
        solverVariant INT NOT NULL,
        FOREIGN KEY(benchmark) REFERENCES Benchmarks(id),
        FOREIGN KEY(solverVariant) REFERENCES SolverVariants(id)
    );
CREATE TABLE Licenses(
        id INTEGER PRIMARY KEY,
        name TEXT,
        link TEXT,
        spdxIdentifier TEXT);
CREATE TABLE Logics(
        logic TEXT PRIMARY KEY,
        quantifierFree BOOL,
        arrays BOOL,
        uninterpretedFunctions BOOL,
        bitvectors BOOL,
        floatingPoint BOOL,
        dataTypes BOOL,
        strings BOOL,
        nonLinear BOOL,
        difference BOOL,
        reals BOOL,
        integers BOOL
        );
-- Tables to store counts of things like interpreted constants, ite, let, etc.
CREATE TABLE Symbols(
        id INT PRIMARY KEY,
        name TEXT);
    );
CREATE TABLE SymbolCounts(
        symbol INT,
        query INT,
        count INT NOT NULL,
        FOREIGN KEY(symbol) REFERENCES Symbols(id)
        FOREIGN KEY(query) REFERENCES Queries (id)
);
CREATE TABLE Solvers(
        id INTEGER PRIMARY KEY,
        name TEXT,
        link TEXT);
-- Since solvers use different versioning schemes, there is
-- no proper version table.  Instead there is only one tables
-- that can be used both for versions, and multiple variants
-- submited to the same competition.
CREATE TABLE SolverVariants(
        id INTEGER PRIMARY KEY,
        fullName TEXT,
        solver INT,
        evaluation INT,
        FOREIGN KEY(solver) REFERENCES Solvers(id)
        FOREIGN KEY(evaluation) REFERENCES Evaluations(id)
        );
-- One entry per stored evaluation (typically SMT-COMPs)
CREATE TABLE Evaluations(
        id INTEGER PRIMARY KEY,
        name TEXT,
        date DATE,
        link TEXT
        );
-- The result of one experiment of an evaluation
CREATE TABLE Results(
        id INTEGER PRIMARY KEY,
        evaluation INTEGER,
        query INT,
        solverVariant INT,
        cpuTime REAL,
        wallclockTime REAL,
        status TEXT,
        FOREIGN KEY(evaluation) REFERENCES Evaluations(id)
        FOREIGN KEY(query) REFERENCES Queries(id)
        FOREIGN KEY(solverVariant) REFERENCES SolverVaraiants(id)
        );
-- Dificulty ratings (see below)
CREATE TABLE Ratings(
        id INTEGER PRIMARY KEY,
        query INT,
        evaluation INT,
        rating REAL,
        consideredSolvers INT,
        successfulSolvers INT,
        FOREIGN KEY(query) REFERENCES Queries(id)
        FOREIGN KEY(evaluation) REFERENCES Evaluations(id)
        );
```

Benchmark difficulty ratings are calculated for each evaluation.  This
calculation first counts the number $n$ of solvers that solved at least one
benchmark in the logic of the benchmark.  Then it count the number $m$ of
solvers that solved the benchmark.  The rating is $1 - m/n$.

## Scripts

* `prepopulate.py` sets up the database file and inserts static data.
* `addbenchmark.py` adds a benchmark to the database file.
* `postprocess.py` adds evaluations, and performs any other operation that
  requires all benchmarks to be in the database.

## TODO

* Support for incremental benchmarks.
